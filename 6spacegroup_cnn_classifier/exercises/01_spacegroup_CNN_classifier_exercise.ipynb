{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edex 1: Determine the space group of a structure from the atomic pair distribution function using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Motivations\n",
    "\n",
    "In this edex, we use a 1D convolutional neural networks model to solve a materials science problem. \n",
    "\n",
    "#### Goal: \n",
    "predict the space group of a crystal structure giving a calculated or measured atomic pair distribution function (PDF) from that structure. We will use 40,000 PDFs that are calculated from 8 of the most common space groups. \n",
    "\n",
    "#### Motivation: \n",
    "Materials scientists are interested in studying structure-property relationships. The ML model would allow us to quickly and easily obtain a list of most likely space groups that can be used for subsequent structural modeling. It allows us to narrow down the range of possible space group at early stage of the research. It also saves a lot of time compared to searching structural databases manually.  \n",
    "\n",
    "#### relevant materials science background \n",
    "- Crystalline materials are consist of repeating arrangement of atoms with long range translational symmetry. The geometric symmetries of crystals are described by space groups. Each space group contains a set of geometric symmetery operations that map a crystal structure back onto itself. As the name suggest, these set of operations form a group in mathematics. For 3D, there are 230 space groups and any crystal structure is described by 1 and only 1 of the 230 possible space groups. \n",
    "- We are interested in determining structure crystalline materials because it is crucial for understanding the materials' properties. The pair distribution function (PDF) analysis is a powerful method for solving crystal structures. PDF is a 1D spatial function which describes the distribution of distances between pairs of particles contained within a given volume. It can be calculated or experimentally obtained from powder diffraction data. However, it is not a simple task to deduce the space group from PDF data because PDF doesn't provide information on the overall symmetry and unit cell of the material. \n",
    "- We know that symmetry information must exist in the PDF, but we do not have a theory yet to identify the space group from PDF. Therefore, ML model is a promising method in this case for deducing the predictive relationship between PDF and space group. We also have considerable amount of data for training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Papers\n",
    "\n",
    "This edex is a simplified version adapted from the papers below:\n",
    "\n",
    "Liu, C. H., Tao, Y., Hsu, D., Du, Q., & Billinge, S. J. (2019). Using a machine learning approach to determine the space group of a structure from the atomic pair distribution function. Acta Crystallographica Section A: Foundations and Advances, 75(4), 633-643.\n",
    "\n",
    "Lan, L., Liu, C. H., Du, Q., & Billinge, S. J. (2022). Robustness test of the spacegroupMining model for determining space groups from atomic pair distribution function data. Journal of Applied Crystallography, 55(3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the problem in machine learning\n",
    "\n",
    "Recall that machine learning can be roughly divided into supervised and unsupervised learning. Supervised learning uses labeled datasets for training, while unsupervised learning uses ML to analyze and cluster unlabeled datasets. Supervised learning can be further categorized into classification problems and regression problems. Classification problems assign input into specific categories. Regression problems aims to understand the relationship between input and output variables, so a regression model predicts numerical values based on the input data.\n",
    "\n",
    "In this problem, we will use around 40,000 PDFs that are calculated from 8 of the most common space groups. Therefore, this is a classification task. The output is an array of length 8 where each entry correspond to the probability being in a space group. The sum of all probabilities in the array should add up to 1.\n",
    "\n",
    "In summary:\n",
    "- input: PDF data (1D array)\n",
    "- output: the probabilities of the crystal being in each spacegroup candidate (1D array of length 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary library packages. If any of these packages are not installed in the local environment, install them using pip or conda in the terminal.  \n",
    "\n",
    "You might need to import more libraries or functions as you work through the problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import random\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, MaxPooling1D, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "First, read the data from csv files\n",
    "* input: \"x.csv\"\n",
    "* label: \"y.csv\"\n",
    "\n",
    "You should have 2 numpy arrays of the following sizes:\n",
    "* X.shape = (50550, 209)\n",
    "* y.shape = (50550,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "y = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to further reshape the X arguments into dimension: (50550, 209, 1), that is, each sample need to be off the shape (steps, input_dim). It is a specific requirement for defining a convolutional neural networks in keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the the data into training and testing data using `train_test_split` from `sklearn.model_selection`. Spefify the following arguments:\n",
    "* `test_size=0.2` determines that testing data is 20% of all the data. \n",
    "* `random_state` is an optional argument which ensures that the splits are the same each time (reproducible). \n",
    "* `shuffle=True` data is shuffled before the split. \n",
    "* `stratify=y` ensures that each target category (the 8 space groups) has proportional representation in the testing data as in the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: \",X_train.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"X_test: \",X_test.shape)\n",
    "print(\"y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, there are 40440 data points in the training data and 10110 data points in testing data. Each input is the PDF data: a vector of shape (209,1). Each target is an integer, which is the space group number of the material. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding\n",
    "\n",
    "For classification tasks, the target labels need to be \"one-hot encoded\".\n",
    "\n",
    "For example, if `target = 14`, then it is converted it into `np.array([0,1,0,0,0,0,0,0])`. \n",
    "\n",
    "To do this, We need to assign an index to the each of 8 space group numbers. The index of each space group number is defined in the numpy array below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_ORDER_CNN = np.array([2,14,15,62,139,194,225,227])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To one-hot encode the labels, we need 2 steps:\n",
    "1. For each target, obtain its index in the squence `SG_ORDER_CNN`. For example [14,15,227,2,14....] should be converted into [1,2,7,0,1....]\n",
    "2. Secondly, use `tf.one_hot` transforms each index number into an array of length 8, where every entry is 0 except for it is 1 at the index position. For example the index 7 should be transformed into [0,0,0,0,0,0,0,7]. Don't forget that python indexing starts with 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot =\n",
    "y_test_one_hot =\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we can try some conventional ML models.  Try whichever you like from what you have learned above, but one that was used in the original work was Logistic Regression."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural networks\n",
    "\n",
    "Convolutional neural networks (CNN) are a special type of artificial neural networks (ANN) that uses the convolution operation in place of general matrix multiplication in at least one of the layers. Unlike a multilayer perceptron model, the use of convolution allows CNNs to utilize the local spatial information in the input. For example, CNNs is able to take into account that close-by pixels encode related information. Furthermore, CNNs enforces translational invariance: for instance, a dog in the left corner of an image will be recognized not so different as a dog in the right corner.  As a result, CNNs have superior performances in many applications such as image and signal recognition and processing.\n",
    "\n",
    "CNNs contain 3 types of layers \n",
    "\n",
    "1. Convolutional layer\n",
    "    * a number of filters of fixed sizes are defined, each filter contains trainable weights. The filter sweeps across the input feature layer taking fixed-size steps called \"stride\". The receptive field is the input feature space overlapping the filter at each position as the filter traverses the feature layer. The output is computed by taking the dot product of filter and all the receptive fields.\n",
    "    * filter is also known as kernels; kernel size refers to the dimension of the filter; channel refers to the number of filters used in the layer. \n",
    "\n",
    "\n",
    "2. Pooling layer\n",
    "    * pooling is a method of regularization. It downsamples the feature space and thus avoids overfitting. \n",
    "    * There are 2 types of pooling layer: average pooling and max pooling\n",
    "    * Similarly to convolutional layer, a filter sweeps across to compute the maximum value or average value of each receptive field.\n",
    "\n",
    "\n",
    "3. Fully-connected layer\n",
    "    * the regular neural networks layers. Used in the last few layers to map to the final outputs.\n",
    "\n",
    "The use of filters not only allows CNNs to identify patterns and possible translational symmetry, but also greatly reduces the number of parameters in the model. CNNs reduces the chances of overfitting since the trainable weights in the filters are reused as filters traverse the feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a machine learning depends on the architecture of the model as well as a number of hyperparameters such as learning rate, loss function, activation function. However, there isn't a general method for determining the optimal set of hyperparameters and architecture. In order to find an optimal model for the scientific problem at hand, we usually first refer to scientific litteratures which solve similar problems and establish a similar architecture. Then, we can perform hyperparameter tuning. One can use basic methods such as grid search, or resort to well-established hyperparameter tuning frameworks, such as [optuna](https://optuna.org/).\n",
    "\n",
    "The architecture in this edex is the same one used in the original paper (Liu et al.). This model is already optimized with hyperparameter tuning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants which we are used for defining the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we define the 1D CNN model. \n",
    "\n",
    "First, we specify the model to be a `keras.Sequential` model. A [Sequential model](https://keras.io/guides/sequential_model/) is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor, which is the case in our problem. Calling `model=Sequential([layer0,layer1,layer2...])` automatically connects the output of layer0 to input of layer1, output of layer1 to input of layer2, etc. There are other ways to define the model without using the sequential method, but in those cases, the output and inputs usually need to be explicitly linked.\n",
    "\n",
    "\n",
    "Then, we add the layers in sequence. The first 2 are given to you:\n",
    "1. Input \n",
    "2. Normalize the input \n",
    "...\n",
    "\n",
    "You need to add the rest of the layers:\n",
    "\n",
    "3. `Conv1D` with 256 channels and kernel size of 32. \n",
    "    * include relu activation function\n",
    "    * padding='same'\n",
    "    * kernel_regularizer = regularizers.l2(l2_lambda)\n",
    "    \n",
    "4. `BatchNormalization`: use the same arguments as the given one\n",
    "\n",
    "5. `Conv1D` with 64 channels and kernel size of 32. \n",
    "    * relu activation\n",
    "    * padding='same'\n",
    "    * kernel_regularizer = regularizers.l2(l2_lambda)\n",
    "    \n",
    "6. `BatchNormalization`:same arguments as before\n",
    "\n",
    "7. `MaxPooling1D`: default setting\n",
    "\n",
    "8. `Dropout` with 0.5 dropout percentage\n",
    "\n",
    "9. `Flatten`\n",
    "\n",
    "10. `Dense` with output dimension = 128\n",
    "    * relu activation \n",
    "    * kernel_regularizer = regularizers.l2(l2_lambda)\n",
    "    \n",
    "11. `BatchNormalization`:same arguments as before\n",
    "\n",
    "12. `Dropout` with 0.5 dropout percentage\n",
    "\n",
    "13. `Dense` with output dimension = 8\n",
    "    * softmax activation \n",
    "    * kernel_regularizer = regularizers.l2(l2_lambda)\n",
    "    \n",
    "Finally, don't forget to return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (209,1)\n",
    "l2_lambda = 1e-5 # regularization parameter\n",
    "num_classes = 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_classifier(input_shape=input_shape, num_classes=num_classes):  \n",
    "    \n",
    "    # Define the sequential model:\n",
    "    model = Sequential()\n",
    "    \n",
    "    # two given layers:\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None))\n",
    "    \n",
    "    \n",
    "    # Add the rest of the layers\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to see Model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) random seed can be introduced for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(0)\n",
    "random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training weights for Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification data set with different class proportions is called imbalanced. \n",
    "\n",
    "Imbalanced dataset could be problematic, because if there are significantly more training data in one class, the  model will spend most of its time optimizing for that one class and not learn enough from samples from the other classes. \n",
    "\n",
    "Our data is imbalanced. The amount of data in each class is encoded in the `size_sg.csv`. We can use a pie chart to visualize the proportions of data in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_sg = pd.read_csv('size_sg.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = size_sg[0].to_numpy()\n",
    "sizes = size_sg[1].to_numpy()\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to counteract the unwanted effects due to data skewness, we introduce training weights. \n",
    "\n",
    "Each class is assigned with a weight equal to the inverse of its proportion (total data points/class data points). We need to encode these weights in the format of a dictionary for use in the training process: {0: ....., 1: ..... .....}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "Callbacks are used to control the training of a model. Callbacks can help us prevent overfitting, visualize our training progress, save checkpoints, etc.\n",
    "\n",
    "In this edex, we include 2 callbacks: [`Early stopping`](https://keras.io/api/callbacks/early_stopping/) and learning rate scheduling. These are optional, but including them can improve model performance. \n",
    "* learning rate scheduler automatically reduce the learning rate after certain number of epochs. For some problems, it can increase performance and accelerate training processes.\n",
    "* Early stopping is form of regularization used to avoid overfitting. Basically it stops the model from training once the model stops improving on the validation data.\n",
    "    * patience: Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a function that takes epoch as an argument, and outputs the corresponding learning rates:\n",
    "* epoch 0-40: lr = 5e-4\n",
    "* epoch 40-60: lr = 5e-5\n",
    "* epoch >60: lr = 5e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Your code\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_callback = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "earlystopping_callback = EarlyStopping(monitor='val_acc',verbose=1,min_delta=0.5,patience=10,baseline=None)\n",
    "\n",
    "callbacks = [lr_schedule_callback, earlystopping_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters\n",
    "\n",
    "Model.compile defines the loss function, metrics, and optimizer.\n",
    "* Category entropy loss is used as the loss function. It is a common choice for classification tasks.\n",
    "* A metric is a function that is used for humans to judge the performance of your model. However, it is different from loss function because it is not used for training.\n",
    "    * TopK Categorical Accuracy calculates the percentage of records for which the targets (non zero yTrue) are in the top K predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documentation for `tf.keras.model.compile` to compile the model.\n",
    "* optimizer: adam\n",
    "* loss function: categorical cross entropy loss\n",
    "* metrics: include 2: accuracy and Top K categorical accuracy with k=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation \n",
    "\n",
    "A validation dataset is a sample of data that is not used directly to train the model, but is used to evaluate the model as it is being trained. It provides a more unbiased evaluation because the model is not directly trained on the validation dataset.\n",
    "\n",
    "However, validation is different from the testing data. The final model is completely unaware of the testing data, whereas we choose the final model based on its performance on the validation dataset. \n",
    "\n",
    "Here the validation data is set to be 20% of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch training\n",
    "\n",
    "batch training means that the model takes one gradient descent step after considering a batch of data points. This greatly accelerates training speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "history = model.fit(X_train, y_train_one_hot,\n",
    "                    epochs=epochs, batch_size=32,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=weights,\n",
    "                    validation_split = 0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens when I increase or decrease the batch size? What's its effect on training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "history_dict = history.history\n",
    "pd.DataFrame(history_dict).to_json('history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model's performance on the testing data. \n",
    "\n",
    "Read the documentation for `tf.keras.model.evaluate` to output model performance for the testing data. We want to find both the loss and accuracy of the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='RED'>YOUR SOLUTION:</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
